# Ethics and Responsibility

Working with AI safety and content filtering means thinking like an attacker **without becoming one**.

These are the principles I follow in my personal safety research:

1. **No distribution of harmful content**  
   I never share or publish generated videos or images that could be considered violent, abusive, or otherwise harmful. All such content stays strictly private and is used only to support safety reports.

2. **No concrete instructions for bypassing filters**  
   In public spaces (like this repository), I do not publish explicit prompts, parameter settings, or detailed recipes that could be used to systematically bypass safety filters.

3. **Responsible disclosure first**  
   When I discover a potential safety failure:
   - I document the behavior and my observations.
   - I contact the relevant organization (e.g., OpenAI Support) and share my findings with them privately.
   - I give them time to respond and improve the system.

4. **Respect for policies and terms of use**  
   I treat the model’s policies and terms of use seriously, even when I am exploring edge cases. The goal is to *strengthen* safety, not to “win” against it.

5. **Human impact awareness**  
   Behind every safety failure there can be a real human impact: trauma, harassment, reputational harm. I try to imagine how generated content could affect real people and use that as a guiding compass.

6. **Educational, not exploitative**  
   Public notes, like the ones in this repo, are written to help engineers, researchers, and other responsible users understand risks and patterns — without revealing concrete exploit paths.

If you are reading this as an engineer or researcher: please treat this repository as a set of high-level intuitions, not as ground truth. My observations are based on one user’s experience and are limited in scope.
